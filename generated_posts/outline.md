In order to build AI agents, and with

that I mean agents as defined by me earlier in the

course, that act autonomously, that create a plan

on their own and that then execute that plan by using

tools, if you wanna build such AI agents,

you of course need to understand how you can expose tools

to a large language model, because it will be a large language model that

powers such an agent. And what's extremely important to

understand here is that the large language model

itself will not use any tool. It will not execute

any code. And I know that it can seem like

it would directly execute some code.

It looks like a smart system that's able to execute code on

its own, right? But that's not what's happening.

Large language models are just spitting out tokens.

That never changes. But you can use this token generation

mechanism to have it request a tool use, and

that's what's actually going to happen.

So, you have a large language model that's running inside of an AI

application. It's always like this.

For example, ChatGPT is such an AI application that

uses a large language model inside. The workflows we took a

look at in the last course section are AI applications that

use large language models in parts of this application.

And AI agents would be another example for AI

applications. And the idea is always that the application

uses a large language model but then does a lot of things in

addition to create that overall AI

experience. For example, an application like ChatGPT

also manages a chat history, and that history is then sent to

the model with every new message to give it the entire

history to work on. It's also that AI

application that would define tools that should be

available to the large language model, so to say.

And these tools are simply functions in the

code written by the developers that wrote the AI

application. And we will write such functions,

such tools ourselves in the next lectures to see how that

actually works. It's then a description of these

functions that's inserted into the prompts that are

sent to the large language model to make the model aware

of the functions of the tools it can

request, because again, it can't execute them itself,

it's just a token generator, but of course it can generate tokens

that describe an intended tool use. And for

that you could add an instruction like this to your prompts, where

you tell the model in the end that if it wants to use the web search

tool, it should output web_search:, and then the search

term it wants to use. Of course, that

may seem like the large language model then is able to

think about that and request it if it makes sense, but in the

end all this is doing is it's increasing the likelihood

of tokens being generated that describe this web

search tool use in this way if the rest of the

prompt that reaches the large language model, which may contain

a message sent by the user of the app, benefits from that

tool use. But it's all just about token generation and about

increasing the likelihood of the tool use

description tokens being generated by the large language model, if you

wanna put it like this. So, then depending

on the rest of the prompt, the large language model may

indeed output web_search: current president

of the USA if the user, for example, asked for that current

president. And because of its training and its

fine-tuning and the overall prompt engineering, the large language

model may output tokens that form

this text in the end, web_search: current president

of the USA. Now the idea is not to show this

text to the user who sent the original message of course, because that's

not an answer to their question. Instead the AI

application that's talking to the large language model would capture

that result and would take a look at it, and it would look

at this result and check whether a tool use is requested,

and it knows how a tool use request looks like because

it was also this AI application, or the developers of this

application to be precise, that added these instructions

on how tools should be requested to the prompts.

So the application would simply check whether one of these instructions was

output by the large language model. And if

such a tool request is detected, then

it's the AI application that invokes the actual

tool, so that invokes the actual function which is also part of the

application, and it would forward that search

term to that function. And then of

course it would be the result of that function call that would be passed

back into the chat history, back into a prompt that's then

sent to the large language model. So the large

language model would then receive an updated chat history that

includes the result of this function execution, of this

tool execution you could say, and then the large language

model generates the final output.

That's how tool use works, and if that looks confusing,

no worries, we'll build it from scratch step by step so that you

understand in detail how that all works, and so

that you're able to build your own tool-aware agents in the

future.